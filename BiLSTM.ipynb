{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8765cfc-e81c-4c3b-9c88-8d0452c01f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting openpyxl\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6a/94/a59521de836ef0da54aaf50da6c4da8fb4072fb3053fa71f052fd9399e7a/openpyxl-3.1.2-py2.py3-none-any.whl (249 kB)\n",
      "\u001b[K     |████████████████████████████████| 249 kB 53.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting et-xmlfile\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/96/c2/3dd434b0108730014f1b96fd286040dc3bcb70066346f7e01ec2ac95865f/et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "681230e2-46d9-45cc-ac9a-1e3261198ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting nltk\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 55.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /environment/miniconda3/lib/python3.7/site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: joblib in /environment/miniconda3/lib/python3.7/site-packages (from nltk) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9d/1e/8eb13233ac58edecdd58aa7de0d5b68fc04f7141891c1934036b0b34890a/regex-2023.6.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (755 kB)\n",
      "\u001b[K     |████████████████████████████████| 755 kB 44.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /environment/miniconda3/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.6.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9b0921-00f6-41d3-827a-e230fee73caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         标题                                                 内容\n",
      "0      岳阳楼记  庆历四年春，滕子京谪守巴陵郡。越明年，政通人和，百废具兴，乃重修岳阳楼，增其旧制，刻唐贤今人...\n",
      "1      醉翁亭记  环滁皆山也。其西南诸峰，林壑尤美，望之蔚然而深秀者，琅琊也。山行六七里，渐闻水声潺潺，而泻出...\n",
      "2     湖心亭看雪  崇祯五年十二月，余住西湖。大雪三日，湖中人鸟声俱绝。是日更定矣，余拏一小舟，拥毳衣炉火，独往...\n",
      "3     智取生辰纲  次日早，起五更，在府里把担仗都摆在厅前。老都管和两个虞候又将一小担财帛，共十一担，拣了十一个...\n",
      "4      范进中举  范进进学回家，母亲、妻子俱各欢喜。正待烧锅做饭，只见他丈人胡屠户，手里拿着一副大肠和一瓶酒，...\n",
      "..      ...                                                ...\n",
      "97       马说  世有伯乐，然后有千里马。千里马常有，而伯乐不常有。故虽有名马，辱于奴隶之手，骈死于槽枥之间，...\n",
      "98   送东阳马生序  余幼时即嗜学。家贫，无从致书以观，每假借于藏书之家，手自笔录，计日以还。天大寒，砚冰坚，手指...\n",
      "99     小石潭记  从小丘西行百二十步，隔篁竹，闻水声，如鸣珮环，心乐之。伐竹取道，下见小潭，水尤清冽。全石以为...\n",
      "100    岳阳楼记  庆历四年春，滕子京谪守巴陵郡。越明年，政通人和，百废具兴，乃重修岳阳楼，增其旧制，刻唐贤今人...\n",
      "101    醉翁亭记  环滁皆山也。其西南诸峰，林壑尤美。望之蔚然而深秀者，琅琊也。山行六七里，渐闻水声潺潺，而泄出...\n",
      "\n",
      "[102 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 查看数据\n",
    "import pandas as pd\n",
    "dataset = pd.read_excel('中小学文言文语料.xlsx')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8dea270-cde7-4b5e-afbc-13b9b8dae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import random\n",
    "import torch.utils.data as Data\n",
    "\n",
    "BATCH_SIZE = 15\n",
    "LR = 1e-4\n",
    "NUM_HIDDENS = 256\n",
    "SEQ_LEN = 40\n",
    "WORD_DIM = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "path = './'\n",
    "chinese_data_path = os.path.join(path , '语料.txt')\n",
    "origin_data , predict_word = [],[]\n",
    "\n",
    "# vocab\n",
    "vocab = []\n",
    "# str to index\n",
    "s2i = dict()\n",
    "# index to str\n",
    "i2s = dict()\n",
    "index = 0\n",
    "\n",
    "def get_vocab(path):\n",
    "    chinese_data_path = path\n",
    "    with open(chinese_data_path , 'r') as f:\n",
    "        # print('loading txt...')\n",
    "        for line in f:\n",
    "            if len(line) >= 41:\n",
    "                # 对？进行特殊处理\n",
    "                line = line.replace(\"?\", \"。\")\n",
    "                line = line.replace(\"？\", \"。\")\n",
    "                predict_word.append(line[20])\n",
    "                d = line[:20]+'?'+line[21:40]\n",
    "                origin_data.append(d)\n",
    "                for word in line:\n",
    "                    if word not in vocab:\n",
    "                        vocab.append(word)\n",
    "    f.close()\n",
    "get_vocab(chinese_data_path)\n",
    "\n",
    "# vocab size\n",
    "vocab.append('?')\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "random.shuffle(vocab)\n",
    "s2i = {word: i for i, word in enumerate(vocab)}\n",
    "i2s = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64e48b9a-5009-4e8d-aadc-e74b974e03f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 26)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 划分数据集\n",
    "class Dataset(Data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x,y = self.data[index]\n",
    "        # print(x,y)\n",
    "        x = [s2i[i] for i in x]\n",
    "        y = s2i[y]\n",
    "        return torch.LongTensor(x), torch.tensor(y)\n",
    "\n",
    "data = list(zip(origin_data,predict_word))\n",
    "random.shuffle(data)\n",
    "train_dataset = Dataset(data[:100])\n",
    "test_dataset = Dataset(data[100:])\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True\n",
    ")\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe5d1b56-ebd9-4c96-9bca-80b9b4a61f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数\n",
    "import nltk\n",
    "EPOCH = 10\n",
    "import torch.nn.functional as F\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=WORD_DIM,\n",
    "            hidden_size=NUM_HIDDENS,\n",
    "            bidirectional=True\n",
    "          )\n",
    "        self.out = nn.Linear(NUM_HIDDENS*2, vocab_size)\n",
    "    def forward(self,x):\n",
    "        # print(x.shape)\n",
    "        # x.shape [batch,seq_len,word_dim]\n",
    "        # state.shape [1,seq_len,word_dim]\n",
    "        h_s = torch.randn((1*2,BATCH_SIZE, NUM_HIDDENS),device=device)\n",
    "        c_s = torch.randn((1*2,BATCH_SIZE, NUM_HIDDENS),device=device)\n",
    "        output,(_,_)= self.lstm(x.transpose(1,0),(h_s,c_s)) # output.shape [seq_len,batch,num_hiddens]\n",
    "        chinese = self.out(output[20])  # chinese.shape [batch,vocab_size]\n",
    "\n",
    "        return chinese\n",
    "\n",
    "def encode(data,embed):\n",
    "    return embed(data)\n",
    "\n",
    "def train_porcess(pred,acc,device):\n",
    "    # pred.shape [batch,vocab_size]\n",
    "    # acc.shape [batch]\n",
    "    # print(pred.shape)\n",
    "    pred = F.softmax(pred, dim=-1)\n",
    "    # print(pred.shape)\n",
    "    pred = pred.argmax(dim=-1).cpu().numpy()\n",
    "    acc = acc.cpu().numpy()\n",
    "    hypothesis = [i2s[i] for i in pred.tolist()]\n",
    "    reference = [i2s[i] for i in acc.tolist()]\n",
    "    BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)\n",
    "    print('预测下一个字:',[i2s[i] for i in pred.tolist()])\n",
    "    print('实际下一个字:',[i2s[i] for i in acc.tolist()])\n",
    "    print('BLEU值为：',BLEUscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c478126-355a-4157-ab33-096fa9ccf8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型实例化\n",
    "bilstm = BiLSTM(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(bilstm.parameters(), lr=LR)\n",
    "embed = nn.Embedding(vocab_size, WORD_DIM, device = device)\n",
    "# 保存整个模型 torch.save(bilstm, \"BiLSTM.pth\")\n",
    "torch.save(bilstm.state_dict(),\"BiLSTM.pth\")\n",
    "output_model = './BiLSTM.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04c3bdde-6ee2-4b6b-944e-1cdae4221116",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "loss = 7.768373012542725\n",
      "预测下一个字: ['饵', '倾', '饵', '绮', '饵', '饵', '饵', '耒', '饵', '饵', '孩', '耒', '耒', '叩', '饵']\n",
      "实际下一个字: ['节', '。', '窥', '日', '檐', '其', '者', '叠', '之', '山', '怒', '学', '孔', '吹', '嬉']\n",
      "BLEU值为： 0\n",
      "准确度为： 0.0\n",
      "\n",
      "\n",
      "epoch = 1\n",
      "loss = 7.780065059661865\n",
      "预测下一个字: ['耒', '雅', '饵', '叩', '饵', '楚', '饵', '耒', '饵', '叩', '饵', '饵', '饵', '饵', '绮']\n",
      "实际下一个字: ['以', '狙', '归', '飞', '应', '与', '，', '孔', '气', '也', '通', '折', '》', '少', '横']\n",
      "BLEU值为： 0\n",
      "准确度为： 0.0\n",
      "\n",
      "\n",
      "epoch = 2\n",
      "loss = 7.6887736320495605\n",
      "预测下一个字: ['攥', '饵', '倾', '饵', '饵', '饵', '/', '山', '辞', '旧', '气', '怒', '楚', '耒', '在']\n",
      "实际下一个字: ['其', '大', '斯', '檐', '山', '其', '者', '公', '伯', '尝', '。', '。', '与', '理', '顷']\n",
      "BLEU值为： 9.257324954728539e-232\n",
      "准确度为： 0.06666666666666667\n",
      "\n",
      "\n",
      "epoch = 3\n",
      "loss = 7.614497184753418\n",
      "预测下一个字: ['叩', '辞', '怒', '，', '叠', '怒', '，', '辞', '耒', '，', '怒', '辞', '怒', '借', '嬉']\n",
      "实际下一个字: ['遍', '菊', '名', '。', '哉', '至', '尝', '借', '学', '。', '更', '政', '怒', '数', '节']\n",
      "BLEU值为： 1.1008876702055895e-231\n",
      "准确度为： 0.06666666666666667\n",
      "\n",
      "\n",
      "epoch = 4\n",
      "loss = 7.574545383453369\n",
      "预测下一个字: ['，', '。', '叠', '，', '辞', '辞', '。', '，', '百', '，', '叠', '伯', '百', '借', '辞']\n",
      "实际下一个字: ['遍', '。', '横', '。', '块', '伯', '。', '斯', ' ', '，', '渭', '百', '州', '行', '理']\n",
      "BLEU值为： 1.384292958842266e-231\n",
      "准确度为： 0.06666666666666667\n",
      "\n",
      "\n",
      "epoch = 5\n",
      "loss = 7.508856773376465\n",
      "预测下一个字: ['，', '。', '。', '，', '。', '，', '辞', '。', '，', '辞', '借', '，', '气', '，', '怒']\n",
      "实际下一个字: [' ', '。', '。', '之', '以', '的', '沉', '无', '莫', '政', '借', '名', '其', '更', '怒']\n",
      "BLEU值为： 5.541564466373977e-155\n",
      "准确度为： 0.06666666666666667\n",
      "\n",
      "\n",
      "epoch = 6\n",
      "loss = 7.427260398864746\n",
      "预测下一个字: ['，', '可', '可', '。', '。', '者', '。', '，', '，', '辞', '，', '。', '。', '，', '。']\n",
      "实际下一个字: ['行', '可', '可', '至', '而', '/', '日', '更', '。', '菊', '，', '州', '飞', '块', '折']\n",
      "BLEU值为： 5.541564466373977e-155\n",
      "准确度为： 0.06666666666666667\n",
      "\n",
      "\n",
      "epoch = 7\n",
      "loss = 7.344034194946289\n",
      "预测下一个字: ['，', '。', '。', '。', '，', '。', '。', '。', '。', '，', '。', '，', '，', '。', '。']\n",
      "实际下一个字: ['的', '折', '窥', '无', '哉', '去', '婴', '。', '来', '，', '以', '骤', '公', '应', '去']\n",
      "BLEU值为： 1.1008876702055895e-231\n",
      "准确度为： 0.0\n",
      "\n",
      "\n",
      "epoch = 8\n",
      "loss = 7.316340446472168\n",
      "预测下一个字: ['，', '。', '。', '。', '。', '。', '。', '。', '。', '，', '。', '。', '。', '。', '。']\n",
      "实际下一个字: ['，', '伯', '中', '一', '横', '》', '哉', '政', '或', '遍', '折', '渭', '州', '怒', '也']\n",
      "BLEU值为： 9.257324954728539e-232\n",
      "准确度为： 0.06666666666666667\n",
      "\n",
      "\n",
      "epoch = 9\n",
      "loss = 7.162171840667725\n",
      "预测下一个字: ['。', '。', '。', '。', '。', '。', '。', '。', '。', '。', '。', '。', '。', '，', '，']\n",
      "实际下一个字: ['以', '继', '檐', '窥', '或', '无', '沉', '去', '归', '。', '名', '空', '吹', '蔚', '的']\n",
      "BLEU值为： 9.257324954728539e-232\n",
      "准确度为： 0.06666666666666667\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "BATCH_SIZE = 15\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    bestscore = 0\n",
    "    # output_model = './BiLSTM.pth'\n",
    "    state_dict = torch.load(output_model, map_location='cpu')\n",
    "    # print(checkpoint)\n",
    "    bilstm.load_state_dict(torch.load(\"BiLSTM.pth\"))\n",
    "    bilstm.to(device)\n",
    "    bilstm.train()\n",
    "    for epoch in range(10):\n",
    "        print(f'epoch = {epoch}')\n",
    "        accuracy = 0\n",
    "        time = 0\n",
    "        for step, datas in enumerate(train_loader):\n",
    "            data, label = tuple(t.to(device)for t in datas)\n",
    "            data = encode(data, embed)\n",
    "            chinese = bilstm(data)\n",
    "            loss = criterion(chinese, label.long())\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            if step%10 == 0:\n",
    "                print(f'loss = {loss}')\n",
    "\n",
    "            if step%50 == 0:\n",
    "                train_porcess(chinese, label, device)\n",
    "        bilstm.eval()\n",
    "        for step, datas in enumerate(test_loader):\n",
    "            # print(datas)\n",
    "            data, label = tuple(t.to(device)for t in datas)\n",
    "            data = encode(data, embed)\n",
    "            chinese = bilstm(data)\n",
    "\n",
    "            pred = F.softmax(chinese, dim=-1)\n",
    "            pred = pred.argmax(dim=-1).cpu().numpy()\n",
    "            acc = label.cpu().numpy()\n",
    "            accuracy += sum(acc==pred)\n",
    "            time += len(acc)\n",
    "            accuracy/=time+0.0\n",
    "        # 百分数形式 print('准确度为：{:.2f}%\\n'.format(accuracy * 100))\n",
    "        print('准确度为：',accuracy)\n",
    "        print('\\n')\n",
    "        if accuracy>bestscore:\n",
    "            torch.save(\n",
    "            {\n",
    "                'model_state_dict': bilstm.state_dict(),\n",
    "                'optimizer_state_dict': optim.state_dict()\n",
    "            }, output_model\n",
    "          )\n",
    "            bestscore = accuracy\n",
    "        bilstm.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
